{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticktack\n",
    "from jax.numpy import array, pi, exp, sin, mean, median, var, arange, sum\n",
    "from numpy import random\n",
    "from pandas import DataFrame\n",
    "from plotnine import ggplot, facet_wrap, labs, aes, theme_bw, geom_tile, scale_color_cmap\n",
    "from os import getcwd, walk\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic question is what is the probability of detecting consecutive events based on the distribution of the data. The first step then will be to determine the distribution of the data. This will be done be resampling the points after the event has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = { # This dictionary contains the units for the fluxes and production function\n",
    "    \"Guttler14\": {  # Units of the Guttler 2014 paper\n",
    "        \"production_rate_units\": \"atoms/cm^2/s\",    # Units of the production rate \n",
    "        \"flow_rate_units\": \"Gt/yr\"                  # Units of the fluxes\n",
    "    },\n",
    "    \"Brehm21\": {    # Units used by the Brehm, et. al. paper\n",
    "        \"production_rate_units\": \"kg/yr\",    # Units of the production rate\n",
    "        \"flow_rate_units\": \"Gt/yr\"           # Units of the fluxes\n",
    "    },\n",
    "    \"Buntgen18\": {  # The units used by the Buntgen 2018 paper\n",
    "        \"production_rate_units\": \"atoms/cm^2/s\",    # Units of the production function\n",
    "        \"flow_rate_units\": \"Gt/yr\"                  # Units of the fluxes \n",
    "    },\n",
    "    \"Miyake17\": {   # The units used by the Miyake 2017 et. al. paper\n",
    "        \"production_rate_units\": \"atoms/cm^2/s\",    # Units of the production function \n",
    "        \"flow_rate_units\": \"1/yr\"                   # Units of the fluxes.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = []  # List for storing the data set locations\n",
    "data_sets_directory = f\"{getcwd()}/datasets\" # Home directory of the data \n",
    "for (root, dirs, files) in walk(data_sets_directory):    # Looping over directories \n",
    "    for file in files:  # Looping through the files \n",
    "        file_path = root + \"/\" + file   # Setting up the path \n",
    "        data_sets.append(file_path)  # Extending the stored directoriess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will also have a `shape` parameter eventually. First however I want to get this running. Damn this really is well suited to a class structure since then I can set `self.set_annual_samples()` need to check if this has actually been implemented. The answer is __No__. I need to add the growth seasons to the `model_units` (which I might just rename `models`). This will lead to ?two? extra field `hemisphere_model` (bool) and `growth_seasons`. Alternatively this could result in a further nested dictionary like `hemispheres = {\"NH_growth\": array([]), \"SH_growth\": array([])}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model: str, datum: str):\n",
    "    cbm = ticktack.load_presaved_model( # Generating the CarbobBoxModel using ticktack\n",
    "        model,  # Name of the model as looped from the models dictionary \n",
    "        production_rate_units=models[model][\"production_rate_units\"], \n",
    "        flow_rate_units=models[model][\"flow_rate_units\"]\n",
    "    )\n",
    "\n",
    "    bayesian_model = ticktack.fitting.SingleFitter(cbm)   # Fitting a model \n",
    "    bayesian_model.prepare_function(model=\"simple_sinusoid\")# Generating the simple sin model\n",
    "    bayesian_model.load_data(datum)   # Loading the data into the model  \n",
    "\n",
    "    return bayesian_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_production_function(model: ticktack.fitting.SingleFitter):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        model: `str` - The `CarbonBoxModel` that is to be used\n",
    "        data: `str` - The dataset that the production function is to be fitted to (.csv)\n",
    "    Returns:\n",
    "        production: `function` - The ideal production function \n",
    "    \"\"\"\n",
    "    # Return to nedler-mead \n",
    "    # Simulate 50 year \"template/burn in\"\n",
    "    # Fit the miyake data with mcmc including sinusoid. \n",
    "    # Not too comfortable with atmospheric dynamics.\n",
    "        # Stratospheric pull down ? radio carbon\n",
    "        # Pulled down close to the surface at mid-year\n",
    "    # ANU carbon models\n",
    "        # Cameron O'neil\n",
    "    # Sharp-rise (Late rise) 774 AD datasets\n",
    "\n",
    "    # The template that I want to pick removes the sinuspoid but uses the event parameters that were fitted with the sinusoid. Using some measure like MLE or mean. That way we are isolating the event. Save this to a file and evaluate on a 50 year grid with the event happening in the middle. \n",
    "\n",
    "    # So split the file into get_template.ipynb -> Run once and save the output onto a 50 year grid. \n",
    "    initial_parameters = array([model.time_data[len(model.time_data) // 2], 1./12, pi/2., 81./12])\n",
    "    # Guess the initial parameters by hand.\n",
    "\n",
    "    # emcee is an implementation of affine invariant ensamble mcmc. Metropolis hastings is the vanilla that has one walker and each evaluation is a step/not step of the walker. You can also do parallel tempering (MH). Ensamble mcmc is something different, you take a hundred points and propose a dist based on the stretch move. The stretch move draws lines between the points and takes a step along the line (from some dist). This results in the thing being affine, which is a generalisation of linear. This is provably optimal for doing mcmc on a multivariate gaussian.\n",
    "\n",
    "    # Need to initialise the parameters as a multi-dimensional gaussian noise, to stetch the lines. This is what the condition number means. The condition number is the ratio of the smallest and largest eigenvalue.\n",
    "\n",
    "    # DFM's website demonstrates how to do this \n",
    "    sampler = emcee.EnsembleSampler(4, 4, model.log_likelihood)\n",
    "\n",
    "    print(\"Running burn-in...\")\n",
    "    p0, _, _ = sampler.run_mcmc(initial_parameters, 200, progress=True);\n",
    "\n",
    "    print(\"Running production...\")\n",
    "    sampler.reset()\n",
    "    sampler.run_mcmc(p0, model.simple_sinusoid, progress=True);\n",
    "    samples = sampler.flatchain\n",
    "\n",
    "    parameters = {  # A dictionary of the model parameters for the `simple_sinusoid`\n",
    "        \"Start Date (yr)\": None,    # The year that the event began \n",
    "        \"Duration (yr)\": None,      # Number of years that the event occured over \n",
    "        \"Phase (yr)\": None,         # The phase shift of the sinusoidal production function \n",
    "        \"Area\": None               #? What are the units?\n",
    "    } \n",
    "\n",
    "    for i, parameter in enumerate(parameters):  # Looping through the parameters \n",
    "        parameters[parameter] = {   # Nested dictionary to store statistical information\n",
    "            \"mean\": mean(samples[:, i]),    # Storing the mean of the samples produced by mcmc\n",
    "            \"median\": median(samples[:, i]),# Storing the median in addition to the mean \n",
    "            \"variance\": var(samples[:, i])  # Storing the variance of the parameter\n",
    "        }\n",
    "    parameters[\"Steady Production\"] = model.steady_state_production   \n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production(t: float, params: dict):\n",
    "    \"\"\"\n",
    "    The best fit production function as estimated using `mcmc`\n",
    "    \"\"\"\n",
    "    middle = params[\"Start Date (yr)\"][\"mean\"] + \\\n",
    "        params[\"Duration (yr)\"][\"mean\"] / 2.0    # Calculating the center of the event\n",
    "    height = params[\"Area\"][\"mean\"] / params[\"Duration (yr)\"][\"mean\"] # The magnitude of the event \n",
    "\n",
    "    gauss = height * exp(- ((t - middle) / (1. / 1.93516 * \\\n",
    "        params[\"Duration (yr)\"][\"mean\"])) ** 16.)   # The super-gaussian event\n",
    "\n",
    "    sine = params[\"Steady Production\"] + \\\n",
    "        0.18 * params[\"Steady Production\"] * \\\n",
    "        sin(2 * pi / 11 * t + params[\"Phase (yr)\"][\"mean\"]) # Sinusoidal component of production \n",
    "    \n",
    "    return sine + gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual_distribution(model: ticktack.fitting.SingleFitter, params: list):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        production: function - The production function, typically determined using `get_production_function`.\n",
    "        data: str - The file name of the data. Should be the same as the file name that was provided to `get_production_function`.\n",
    "    Returns:\n",
    "        An `mcmc` sample of the posterior distribution of the residuals, which has been fitted with a parameteric (to start) distribution that can be used to simulate noise.\n",
    "    \"\"\"\n",
    "    # Not needed right away. \n",
    "    # DFM's is the maximum level IR\n",
    "    # The simplest way:\n",
    "        # For each year take a 50 (flexible) year chunk of real data from Intcal20.\n",
    "        # We do not want to use Intcal20 because the data is interpolated to be on the same grid although they are not. \n",
    "        # We have that the data. #! Get of Uratkash\n",
    "        #? Is there a spike in the midyear?\n",
    "        # We determine this be comparing to the ideal simulated data (Template)\n",
    "        # Calculate the difference in chi sqaured between the data and a perfectly straight line and the simulated event. \n",
    "            #* Long term increase can be fitted with linear regression.  \n",
    "            # Long term might use optimisation, gp or ea\n",
    "\n",
    "    # Evolutionary algorithms are a method of optimasation.\n",
    "        # Imagine there is a bunch of different combinations of parameters that map to a genome. calculate the objective function for each genome. which is the fitness of the model. The ones with the worst performance are killed breading the best performing models.\n",
    "        # Efficient for difficult optimisation problems.\n",
    "        # Hybrid fitness through iteration. \n",
    "        # Alsiong wong\n",
    "    \n",
    "    # Gaussian processes are also useful in optimisation\n",
    "        # A way of representing on-parametric function as a kernel is a distribution over functions. (Hilber space?)\n",
    "        # The covariance matrix of the multi-dimensional sampling space is parametrised (as the kernel). \n",
    "        # The covariance is itself a function of the independent variable. \n",
    "        # About the statistics of functions\n",
    "\n",
    "    #! The Mackie book has chapters on both\n",
    "    # There is also differential evolution. \n",
    "\n",
    "    # Bayesian information criterion and the Akaike tell us the worth of additional parameters. Callibrate for each year for computational feesible run times. Resample the noise and see how often you get different chi squared differences / BICs and Akaikes. This is a histogram of the test statistic and get the 95% confidence level and how often would there be a false positive. Both false positives and false negitives are of interest. (The independent variable is the event size.)\n",
    "\n",
    "    # ?Bootstrapping\n",
    "\n",
    "    # Using this table of the test statistic. What test stat value is the best?\n",
    "    # Repeat this for every year and therefore know what event size to look for in each year. \n",
    "    # This will be a big grid. This is a table of sensitivities. For each event given its parameters calculate the probablity of finding that event. If one has a 0.5 chance then there were two of them.\n",
    "\n",
    "    #? Per Bak has a book \"How nature works: The science of self organised criticality.\"\n",
    "    \n",
    "    # The dist of miyake events will have some distribution that might be one over f\n",
    "    # This will mean that the 6 main events tell you something about the distribution of small events. \n",
    "    # Ultimately this is where we want to go.\n",
    "\n",
    "    residuals = model.d14c_data - model.dc14(params)\n",
    "\n",
    "    gaussian_error_parameters = {   # Dictionary containing the parameters of a parametric gaussian\n",
    "        \"mean\": mean(residuals),    # The mean of the residuals assumed to have gaussian error\n",
    "        \"variance\": var(residuals)  # Variance of the residuals assumed to have gaussian error\n",
    "    }\n",
    "\n",
    "    return gaussian_error_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the error in the data measurements will have a distribution that I can use to generate imaginary error in the data. I'm not sure how this will help but it could change the way things shape out so I will implement this and keep track of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_error(model: ticktack.fitting.SingleFitter, error: dict, theoretical_dc14: array):\n",
    "    \"\"\"\n",
    "    Simulates a Miyake event based on the things that have already transpired. \n",
    "    \"\"\"   \n",
    "    size = len(model.d14c_data) # Size of the model \n",
    "    random_error = random.randn(size) * error[\"variance\"] + error[\"mean\"] # Generating the noise \n",
    "\n",
    "    data = {    # A Dictionary that I will convert to a DataFrame and return \n",
    "        \"Year\": model.time_data,        # The time series data \n",
    "        \"DC14\": theoretical_dc14 + random_error,           # Simulated C14 data\n",
    "    }\n",
    "\n",
    "    return DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_event(event: DataFrame, EDC14: array):\n",
    "    \"\"\"\n",
    "    Used to determine how likely an event is to have occured. \n",
    "    Parameters:\n",
    "        event: `DataFrame` -> A simulated event\n",
    "        EDC14: `array` -> The theoretical values \n",
    "    Returns:\n",
    "        The chi-squared of the simulated event given the EDC14 and the error in the chi-squared\n",
    "    \"\"\"\n",
    "    # The first line should be a linear best fit\n",
    "    # Then calculate the chi squared and the test statistic is the difference between that and the template.\n",
    "\n",
    "    chi_squared = (array([*event[\"DC14\"]]) - EDC14) ** 2 / array([*event[sig_DC14]]) ** 2  # Calculates generalised chi squared\n",
    "    # chi_squared_error = 2 * array([*event[\"Sig_DC14\"]])   # Error propagation of chi-sqaured \n",
    "    return float(sum(chi_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = {model: models[model] for model in [\"Guttler14\", \"Miyake17\"]} # Extracting two test models\n",
    "test_data = data_sets[:1]   # Test data sets to run the test models on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = {         # Holds the data farmed from the simulation \n",
    "    \"Model\": [],        # The model that was used in the simulation\n",
    "    \"Data\": [],         # Holds the name of the dataset\n",
    "    \"Height\": [],       # The peak height of the production function \n",
    "    \"Duration\": [],     # The duration of the productioin function \n",
    "    \"Chi Squared\": []   # The chi-squared of the model \n",
    "}\n",
    "\n",
    "for model in test_models:   # Looping over the models \n",
    "    for datum in test_data: # Looping over the data\n",
    "        datum_str = datum.split(\"/\")[-1]    # Just storing the csv name not the whole adress path \n",
    "        \n",
    "        print(  \n",
    "            f\"Current Model = {model} \\n\" +\n",
    "            f\"Current Datum = {datum_str}\"\n",
    "        )\n",
    "\n",
    "        model_obj = get_model(model, datum) # Loading the model into RAM\n",
    "        model_obj.production = production   # Assigning the production function \n",
    "\n",
    "        production_params = get_production_function(model_obj)   # Getting the production function \n",
    "        data_error = get_residual_distribution(model_obj, [production_params])  # Getting error \n",
    "\n",
    "        for height in arange(0.1, 8.0, 0.1):   # Looping over a range of areas \n",
    "            for duration in arange(0.1, 8.0, 0.1):   # Looping over a range of durations\n",
    "                event_params = production_params.copy()             # Copying the parameters \n",
    "                event_params[\"Area\"][\"mean\"] = height * duration    # Updating the area parameter\n",
    "                event_params[\"Duration (yr)\"][\"mean\"] = duration    # Changing the duration \n",
    "\n",
    "                theoretical_dc14 = model_obj.dc14([event_params])      # Running the model \n",
    "                event = simulate_error(model_obj, data_error, theoretical_dc14)  # Running simulation \n",
    "                chi_squared = recover_event(event, theoretical_dc14) # getting shi \n",
    "\n",
    "                simulations[\"Data\"].append(datum_str)           # Storing the data\n",
    "                simulations[\"Model\"].append(model)              # Storing the model \n",
    "                simulations[\"Height\"].append(height)            # Storing the simulation area\n",
    "                simulations[\"Duration\"].append(duration)        # Storing the simulations duration \n",
    "                simulations[\"Chi Squared\"].append(chi_squared)  # Storing the chi-squared\n",
    "\n",
    "        print(\"\\n\") # New line for nicer terminal display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is starting to come together. I need to work out what is causing the huge variance in the chi-squared though. I might do this on some smaller meshes. \n",
    "I also might want to plot all the data sets so that I can see what I am dealing with. I'll make this the first goal for after lunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = DataFrame(simulations)\n",
    "(ggplot(simulations, aes(x=\"Height\", y=\"Duration\", fill=\"Chi Squared\"))\n",
    "    + geom_tile()\n",
    "    + scale_color_cmap(cmap_name=\"Spectral\")\n",
    "    + facet_wrap(\"~Model + Data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel density information will enable me to get the maximum likelhood as well as the mean and median."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c4d728f571dc14aecd61e9c5a335d0795680d56501238c7b0344daa2ef0c43f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ticktack': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
