{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 \n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(passed_html):\n",
    "    \"\"\"\n",
    "    Passed a GitHub-Repo this function finds all of the child directories under that repo and returns a list of urls.\n",
    "    \"\"\"\n",
    "    sub_directories = passed_html.find_all(\"a\", class_=\"js-navigation-open Link--primary\") # Finding all of the internal navigation links\n",
    "\n",
    "    repositories = []   # A list to store the new repository urls.\n",
    "    for sub_directory in sub_directories:   # Looping through the child directories as strings \n",
    "        for js_string in str(sub_directory).split('\"'): # Retrieving the java-script strings \n",
    "            if js_string.startswith(\"/\"):   # Finding the internal links by the first character \n",
    "                repositories.append(\"\".join([\"https://github.com\", js_string])) # Adding the appropriate preface to the url.\n",
    "\n",
    "    return repositories # Returning the list of internal directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_request = requests.get(\"https://github.com/SharmaLlama/ticktack/tree/main/src/data/datasets\") # Getting the root directory\n",
    "html_parser = bs4.BeautifulSoup(http_request.text, \"html.parser\")   # Passing the raw text of the directy as html for better manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = []  # Initialising a list to store the directories\n",
    "for dir in get_directories(html_parser):    # Finding all the child directories of the root directory \n",
    "    data_set = requests.get(dir)    # Getting the raw http response from the child directory \n",
    "    passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the child directory as html \n",
    "\n",
    "    data_sets.append(get_directories(passed_data))  # Storing the children's children in the data_sets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, directory_data_sets in enumerate(data_sets): # Looping over the data sets\n",
    "    for data_set_url in directory_data_sets:    # getting the urls of the data_sets\n",
    "        if data_set_url.find(\"csv\") == -1:  # Checking that the leaves of the directory have been reached \n",
    "            data_set = requests.get(data_set_url)   # Getting a response from the data set repo\n",
    "            passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the data set as\n",
    "            data_sets[i] = get_directories(passed_data) # Updating the stored entry to make the list homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []   # List to store the python data sets\n",
    "\n",
    "for data_urls in data_sets: # Loopig through the data sets in found by the program \n",
    "    for data_url in data_urls:  # Extracting the individual urls \n",
    "        data_set = requests.get(data_url)   # getting the http response for the data\n",
    "        data_set = bs4.BeautifulSoup(data_set.text, \"html.parser\")  # Passing the data html \n",
    "\n",
    "        datum = {    # Storing the data as a python dictionary \n",
    "            \"year\": [], # A column to store the year \n",
    "            \"d14c\": [], # A column to store the carbon 14 concentrations \n",
    "            \"sig_d14c\": []  # A columns to store the error in the carbon 14 concentrations \n",
    "        }\n",
    "        \n",
    "        tabular_rows = data_set.find_all(\"tr\") # retrieving the tabular data \n",
    "        columns_names = [\"year\", \"d14c\", \"sig_d14c\"]\n",
    "\n",
    "        for tabular_row in tabular_rows:  # Looping through each line of the table \n",
    "            column_counter = 0  # Keeping track of the current column in the data \n",
    "            tabular_columns = tabular_row.find_all(\"td\")    # Finding all of the entries in the row\n",
    "\n",
    "            for tabular_column in tabular_columns:  # looping over the columns \n",
    "                if tabular_column.has_attr(\"id\"):   # Checking if the searcher is in a data column \n",
    "                    column_counter = 0  # Resetting the column indicating new row\n",
    "                else:\n",
    "                    tabular_entry = str(tabular_column).strip(\"<td>\").strip(\"</td>\")   # Retrieving data\n",
    "                    datum[columns_names[column_counter]].append(tabular_entry)    # Adding the data to storage \n",
    "                    column_counter += 1 # Moving along one column\n",
    "                \n",
    "        data.append(datum)\n",
    "\n",
    "        # pandas.DataFrame(datum).to_csv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all of the data sets, are lining up. I need to improve this with a conditional statement and then get the data stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'year': '-5246', 'd14c': '98.94', 'sig_d14c': '2.26'},\n",
       " {'year': '-5247', 'd14c': '97.46', 'sig_d14c': '1.71'},\n",
       " {'year': '-5247', 'd14c': '96.62', 'sig_d14c': '1.77'},\n",
       " {'year': '-5251', 'd14c': '99.6', 'sig_d14c': '1.87'},\n",
       " {'year': '-5396', 'd14c': '86.9', 'sig_d14c': '3.4'},\n",
       " {'year': '-5400', 'd14c': '91.7', 'sig_d14c': '3'},\n",
       " {'year': '-5381', 'd14c': '84.9', 'sig_d14c': '2.3'},\n",
       " {'year': '-633', 'd14c': '-0.3', 'sig_d14c': '2.4'},\n",
       " {'year': '-633', 'd14c': '2.4', 'sig_d14c': '2.3'},\n",
       " {'year': '-7147', 'd14c': '82.91', 'sig_d14c': '2.95'},\n",
       " {'year': '-7147', 'd14c': '83.17', 'sig_d14c': '1.92'},\n",
       " {'year': '-7150', 'd14c': '84.33', 'sig_d14c': '2.47'},\n",
       " {'year': '780', 'd14c': '-8.8', 'sig_d14c': '1.4'},\n",
       " {'year': '780', 'd14c': '-12.9', 'sig_d14c': '1.6'},\n",
       " {'year': '780', 'd14c': '-12.1', 'sig_d14c': '1.6'},\n",
       " {'year': '780', 'd14c': '-11.6', 'sig_d14c': '2.3'},\n",
       " {'year': '780', 'd14c': '-13.8', 'sig_d14c': '2.3'},\n",
       " {'year': '780', 'd14c': '-12.4', 'sig_d14c': '2.3'},\n",
       " {'year': '1000', 'd14c': '-15.19', 'sig_d14c': '2.40'},\n",
       " {'year': '1000', 'd14c': '-13.64', 'sig_d14c': '2.36'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c4d728f571dc14aecd61e9c5a335d0795680d56501238c7b0344daa2ef0c43f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ticktack': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
