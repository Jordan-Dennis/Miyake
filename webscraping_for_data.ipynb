{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 \n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(passed_html):\n",
    "    \"\"\"\n",
    "    Passed a GitHub-Repo this function finds all of the child directories under that repo and returns a list of urls.\n",
    "    \"\"\"\n",
    "    sub_directories = passed_html.find_all(\"a\", class_=\"js-navigation-open Link--primary\") # Finding all of the internal navigation links\n",
    "\n",
    "    repositories = []   # A list to store the new repository urls.\n",
    "    for sub_directory in sub_directories:   # Looping through the child directories as strings \n",
    "        for js_string in str(sub_directory).split('\"'): # Retrieving the java-script strings \n",
    "            if js_string.startswith(\"/\"):   # Finding the internal links by the first character \n",
    "                repositories.append(\"\".join([\"https://github.com\", js_string])) # Adding the appropriate preface to the url.\n",
    "\n",
    "    return repositories # Returning the list of internal directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_request = requests.get(\"https://github.com/SharmaLlama/ticktack/tree/main/src/data/datasets\") # Getting the root directory\n",
    "html_parser = bs4.BeautifulSoup(http_request.text, \"html.parser\")   # Passing the raw text of the directy as html for better manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = []  # Initialising a list to store the directories\n",
    "for dir in get_directories(html_parser):    # Finding all the child directories of the root directory \n",
    "    data_set = requests.get(dir)    # Getting the raw http response from the child directory \n",
    "    passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the child directory as html \n",
    "\n",
    "    data_sets.append(get_directories(passed_data))  # Storing the children's children in the data_sets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, directory_data_sets in enumerate(data_sets): # Looping over the data sets\n",
    "    for data_set_url in directory_data_sets:    # getting the urls of the data_sets\n",
    "        if data_set_url.find(\"csv\") == -1:  # Checking that the leaves of the directory have been reached \n",
    "            data_set = requests.get(data_set_url)   # Getting a response from the data set repo\n",
    "            passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the data set as\n",
    "            data_sets[i] = get_directories(passed_data) # Updating the stored entry to make the list homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_urls in data_sets: # Loopig through the data sets in found by the program \n",
    "    for data_url in data_urls:  # Extracting the individual urls \n",
    "        data_set = requests.get(data_url)   # getting the http response for the data\n",
    "        data_set = bs4.BeautifulSoup(data_set.text, \"html.parser\")  # Passing the data html \n",
    "        file_name = data_url.split(\"/\")[-1]\n",
    "\n",
    "        datum = {    # Storing the data as a python dictionary \n",
    "            \"year\": [], # A column to store the year \n",
    "            \"d14c\": [], # A column to store the carbon 14 concentrations \n",
    "            \"sig_d14c\": []  # A columns to store the error in the carbon 14 concentrations \n",
    "        }\n",
    "        \n",
    "        tabular_rows = data_set.find_all(\"tr\") # retrieving the tabular data \n",
    "        columns_names = [\"year\", \"d14c\", \"sig_d14c\"]\n",
    "\n",
    "        for tabular_row in tabular_rows:  # Looping through each line of the table \n",
    "            column_counter = 0  # Keeping track of the current column in the data \n",
    "            tabular_columns = tabular_row.find_all(\"td\")    # Finding all of the entries in the row\n",
    "\n",
    "            for tabular_column in tabular_columns:  # looping over the columns \n",
    "                if tabular_column.has_attr(\"id\"):   # Checking if the searcher is in a data column \n",
    "                    column_counter = 0  # Resetting the column indicating new row\n",
    "                else:\n",
    "                    tabular_entry = str(tabular_column).strip(\"<td>\").strip(\"</td>\")   # Retrieving data\n",
    "                    datum[columns_names[column_counter]].append(float(tabular_entry))    # Adding the data to storage \n",
    "                    column_counter += 1 # Moving along one column\n",
    "                \n",
    "        pandas.DataFrame(datum).to_csv(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all of the data sets, are lining up. I need to improve this with a conditional statement and then get the data stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Brehm21_AlpineLarch.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sets[0][0].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_csv in module pandas.core.generic:\n",
      "\n",
      "to_csv(self, path_or_buf: 'FilePathOrBuffer[AnyStr] | None' = None, sep: 'str' = ',', na_rep: 'str' = '', float_format: 'str | None' = None, columns: 'Sequence[Hashable] | None' = None, header: 'bool_t | list[str]' = True, index: 'bool_t' = True, index_label: 'IndexLabel | None' = None, mode: 'str' = 'w', encoding: 'str | None' = None, compression: 'CompressionOptions' = 'infer', quoting: 'int | None' = None, quotechar: 'str' = '\"', line_terminator: 'str | None' = None, chunksize: 'int | None' = None, date_format: 'str | None' = None, doublequote: 'bool_t' = True, escapechar: 'str | None' = None, decimal: 'str' = '.', errors: 'str' = 'strict', storage_options: 'StorageOptions' = None) -> 'str | None'\n",
      "    Write object to a comma-separated values (csv) file.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path_or_buf : str or file handle, default None\n",
      "        File path or object, if None is provided the result is returned as\n",
      "        a string.  If a non-binary file object is passed, it should be opened\n",
      "        with `newline=''`, disabling universal newlines. If a binary\n",
      "        file object is passed, `mode` might need to contain a `'b'`.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "           Support for binary file objects was introduced.\n",
      "    \n",
      "    sep : str, default ','\n",
      "        String of length 1. Field delimiter for the output file.\n",
      "    na_rep : str, default ''\n",
      "        Missing data representation.\n",
      "    float_format : str, default None\n",
      "        Format string for floating point numbers.\n",
      "    columns : sequence, optional\n",
      "        Columns to write.\n",
      "    header : bool or list of str, default True\n",
      "        Write out the column names. If a list of strings is given it is\n",
      "        assumed to be aliases for the column names.\n",
      "    index : bool, default True\n",
      "        Write row names (index).\n",
      "    index_label : str or sequence, or False, default None\n",
      "        Column label for index column(s) if desired. If None is given, and\n",
      "        `header` and `index` are True, then the index names are used. A\n",
      "        sequence should be given if the object uses MultiIndex. If\n",
      "        False do not print fields for index names. Use index_label=False\n",
      "        for easier importing in R.\n",
      "    mode : str\n",
      "        Python write mode, default 'w'.\n",
      "    encoding : str, optional\n",
      "        A string representing the encoding to use in the output file,\n",
      "        defaults to 'utf-8'. `encoding` is not supported if `path_or_buf`\n",
      "        is a non-binary file object.\n",
      "    compression : str or dict, default 'infer'\n",
      "        If str, represents compression mode. If dict, value at 'method' is\n",
      "        the compression mode. Compression mode may be any of the following\n",
      "        possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If\n",
      "        compression mode is 'infer' and `path_or_buf` is path-like, then\n",
      "        detect compression mode from the following extensions: '.gz',\n",
      "        '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given\n",
      "        and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as\n",
      "        one of the above, other entries passed as\n",
      "        additional compression options.\n",
      "    \n",
      "        .. versionchanged:: 1.0.0\n",
      "    \n",
      "           May now be a dict with key 'method' as compression mode\n",
      "           and other entries as additional compression options if\n",
      "           compression mode is 'zip'.\n",
      "    \n",
      "        .. versionchanged:: 1.1.0\n",
      "    \n",
      "           Passing compression options as keys in dict is\n",
      "           supported for compression modes 'gzip' and 'bz2'\n",
      "           as well as 'zip'.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "            Compression is supported for binary file objects.\n",
      "    \n",
      "        .. versionchanged:: 1.2.0\n",
      "    \n",
      "            Previous versions forwarded dict entries for 'gzip' to\n",
      "            `gzip.open` instead of `gzip.GzipFile` which prevented\n",
      "            setting `mtime`.\n",
      "    \n",
      "    quoting : optional constant from csv module\n",
      "        Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format`\n",
      "        then floats are converted to strings and thus csv.QUOTE_NONNUMERIC\n",
      "        will treat them as non-numeric.\n",
      "    quotechar : str, default '\\\"'\n",
      "        String of length 1. Character used to quote fields.\n",
      "    line_terminator : str, optional\n",
      "        The newline character or character sequence to use in the output\n",
      "        file. Defaults to `os.linesep`, which depends on the OS in which\n",
      "        this method is called ('\\\\n' for linux, '\\\\r\\\\n' for Windows, i.e.).\n",
      "    chunksize : int or None\n",
      "        Rows to write at a time.\n",
      "    date_format : str, default None\n",
      "        Format string for datetime objects.\n",
      "    doublequote : bool, default True\n",
      "        Control quoting of `quotechar` inside a field.\n",
      "    escapechar : str, default None\n",
      "        String of length 1. Character used to escape `sep` and `quotechar`\n",
      "        when appropriate.\n",
      "    decimal : str, default '.'\n",
      "        Character recognized as decimal separator. E.g. use ',' for\n",
      "        European data.\n",
      "    errors : str, default 'strict'\n",
      "        Specifies how encoding and decoding errors are to be handled.\n",
      "        See the errors argument for :func:`open` for a full list\n",
      "        of options.\n",
      "    \n",
      "        .. versionadded:: 1.1.0\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None or str\n",
      "        If path_or_buf is None, returns the resulting csv format as a\n",
      "        string. Otherwise returns None.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    read_csv : Load a CSV file into a DataFrame.\n",
      "    to_excel : Write DataFrame to an Excel file.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'],\n",
      "    ...                    'mask': ['red', 'purple'],\n",
      "    ...                    'weapon': ['sai', 'bo staff']})\n",
      "    >>> df.to_csv(index=False)\n",
      "    'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n'\n",
      "    \n",
      "    Create 'out.zip' containing 'out.csv'\n",
      "    \n",
      "    >>> compression_opts = dict(method='zip',\n",
      "    ...                         archive_name='out.csv')  # doctest: +SKIP\n",
      "    >>> df.to_csv('out.zip', index=False,\n",
      "    ...           compression=compression_opts)  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pandas.DataFrame.to_csv)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c4d728f571dc14aecd61e9c5a335d0795680d56501238c7b0344daa2ef0c43f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ticktack': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
