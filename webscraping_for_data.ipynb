{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(passed_html):\n",
    "    \"\"\"\n",
    "    Passed a GitHub-Repo this function finds all of the child directories under that repo and returns a list of urls.\n",
    "    \"\"\"\n",
    "    sub_directories = passed_html.find_all(\"a\", class_=\"js-navigation-open Link--primary\") # Finding all of the internal navigation links\n",
    "\n",
    "    repositories = []   # A list to store the new repository urls.\n",
    "    for sub_directory in sub_directories:   # Looping through the child directories as strings \n",
    "        for js_string in str(sub_directory).split('\"'): # Retrieving the java-script strings \n",
    "            if js_string.startswith(\"/\"):   # Finding the internal links by the first character \n",
    "                repositories.append(\"\".join([\"https://github.com\", js_string])) # Adding the appropriate preface to the url.\n",
    "\n",
    "    return repositories # Returning the list of internal directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "http_request = requests.get(\"https://github.com/SharmaLlama/ticktack/tree/main/src/data/datasets\") # Getting the root directory\n",
    "html_parser = bs4.BeautifulSoup(http_request.text, \"html.parser\")   # Passing the raw text of the directy as html for better manipulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = []  # Initialising a list to store the directories\n",
    "for dir in get_directories(html_parser):    # Finding all the child directories of the root directory \n",
    "    data_set = requests.get(dir)    # Getting the raw http response from the child directory \n",
    "    passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the child directory as html \n",
    "\n",
    "    data_sets.append(get_directories(passed_data))  # Storing the children's children in the data_sets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, directory_data_sets in enumerate(data_sets): # Looping over the data sets\n",
    "    for data_set_url in directory_data_sets:    # getting the urls of the data_sets\n",
    "        if data_set_url.find(\"csv\") == -1:  # Checking that the leaves of the directory have been reached \n",
    "            data_set = requests.get(data_set_url)   # Getting a response from the data set repo\n",
    "            passed_data = bs4.BeautifulSoup(data_set.text, \"html.parser\")   # Passing the data set as\n",
    "            data_sets[i] = get_directories(passed_data) # Updating the stored entry to make the list homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_urls in data_sets: # Loopig through the data sets in found by the program \n",
    "    for data_url in data_urls:  # Extracting the individual urls \n",
    "        data_set = requests.get(data_url)   # getting the http response for the data\n",
    "        data_set = bs4.BeautifulSoup(data_set.text, \"html.parser\")  # Passing the data html \n",
    "\n",
    "        line_number_tags = data_set.find_all(\"td\", class_=\"blob-num js-line-number\")    # Finding the number of line in the html table that the data is displayed in \n",
    "        number_of_lines = int(line_number_tags[-1][\"id\"][1:]) - 1   # Getting the total number of lines as python would see it using the range function \n",
    "\n",
    "        data = {    # Storing the data as a python dictionary \n",
    "            \"year\": [], # A column to store the year \n",
    "            \"d14c\": [], # A column to store the carbon 14 concentrations \n",
    "            \"sig_d14c\": []  # A columns to store the error in the carbon 14 concentrations \n",
    "        }\n",
    "        \n",
    "        tabular_data = data_set.find_all(\"td\")[:-1] # retrieving the tabular data \n",
    "\n",
    "        for line_number in range(number_of_lines * 4):  # Looping through each line of the table \n",
    "            column = line_number % 4    # Finding the column of the present namespace \n",
    "            if column == 1: # This represents the internal line markers \n",
    "                pass\n",
    "            elif column == 2:   # This represents the year of the row \n",
    "                data[\"year\"].append(tabular_data[line_number]) \n",
    "            elif column == 3:   # This represents the carbon content of the row \n",
    "                data[\"d14c\"].append(tabular_data[line_number])\n",
    "            elif column == 0:   # The represents the carbon error of the row \n",
    "                data[\"sig_d14c\"].append(tabular_data[line_number])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c4d728f571dc14aecd61e9c5a335d0795680d56501238c7b0344daa2ef0c43f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ticktack': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
